{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae8047f-8539-4527-b24f-0aa1844ab8e2",
   "metadata": {},
   "source": [
    "# Evaluation of SetFitABSA and Deberta Models\n",
    "\n",
    "Summarization of results from manual evaluation and Comparison of results between SetFitABSA and Deberta models aspect based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d057016-fa6b-4289-92af-ccf083e8098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e133ec-3585-4655-881d-6c7e960764a5",
   "metadata": {},
   "source": [
    "## Evaluation of Deberta Model results\n",
    "\n",
    "Here we are synthesizing the results from the Deberta model along with the manual evaluation done on 100 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8daa1976-2840-4f3f-b120-3d335f0389a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Model Label</th>\n",
       "      <th>Manual Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ifw5wqcChnL4zBigtR7NKA</td>\n",
       "      <td>_WilyI_mvxoVBoHn7crnCQ</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.631905</td>\n",
       "      <td>Space</td>\n",
       "      <td>Super disappointed in this place! Their servic...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id               review_id     Label     Score Aspect  \\\n",
       "0  Ifw5wqcChnL4zBigtR7NKA  _WilyI_mvxoVBoHn7crnCQ  Negative  0.631905  Space   \n",
       "\n",
       "                                             Reviews Model Label Manual Label  \n",
       "0  Super disappointed in this place! Their servic...     Neutral      Neutral  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set which includes both manual evaluated results and the model labeled results\n",
    "deberta_eval_df = pd.read_csv('deberta_manual_eval.csv')\n",
    "\n",
    "# let's look at the top row. For a given aspect, if the confidence score is under 0.7 then model lable is set to neutral. \n",
    "# If the confidence score is more than 0.7 then the model label is set to the 'Label' column\n",
    "# In the row that is printed below, given that the confidence score is under 0.7, the model label is 'Neutral'\n",
    "deberta_eval_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bc44fe-a7f4-4b67-afab-c192d28fad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Manual Label</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Manual Label  Negative  Neutral  Positive\n",
       "Model Label                              \n",
       "Negative          11.0      1.0       0.0\n",
       "Neutral            3.0     14.0       5.0\n",
       "Positive           0.0     11.0      55.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the dataframe to create a confusion matrix with model label and manual label\n",
    "deberta_eval_gp = deberta_eval_df.groupby(['Model Label','Manual Label'])['review_id'].count().reset_index()\n",
    "deberta_eval_gp = deberta_eval_gp.pivot(index='Model Label', columns='Manual Label')['review_id'].fillna(0)\n",
    "\n",
    "deberta_eval_gp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fd6ae0-45a2-466c-9e44-c3c713c87917",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mimshow(deberta_eval_gp, x\u001b[38;5;241m=\u001b[39mdeberta_eval_gp\u001b[38;5;241m.\u001b[39mcolumns, y\u001b[38;5;241m=\u001b[39mdeberta_eval_gp\u001b[38;5;241m.\u001b[39mindex, \\\n\u001b[1;32m      3\u001b[0m                 color_continuous_scale\u001b[38;5;241m=\u001b[39mpx\u001b[38;5;241m.\u001b[39mcolors\u001b[38;5;241m.\u001b[39msequential\u001b[38;5;241m.\u001b[39mGreens, text_auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/plotly/basedatatypes.py:3410\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/plotly/io/_renderers.py:386\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Mimetype renderers\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m bundle \u001b[38;5;241m=\u001b[39m \u001b[43mrenderers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_mime_bundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderers_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bundle:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ipython_display:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/plotly/io/_renderers.py:294\u001b[0m, in \u001b[0;36mRenderersConfig._build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(renderer, k):\n\u001b[1;32m    292\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(renderer, k, v)\n\u001b[0;32m--> 294\u001b[0m         bundle\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_mimebundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig_dict\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bundle\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/plotly/io/_base_renderers.py:126\u001b[0m, in \u001b[0;36mImageRenderer.to_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_mimebundle\u001b[39m(\u001b[38;5;28mself\u001b[39m, fig_dict):\n\u001b[0;32m--> 126\u001b[0m     image_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb64_encode:\n\u001b[1;32m    137\u001b[0m         image_str \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_bytes)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/plotly/io/_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "# Showing the image using Plotly express heatmap\n",
    "fig = px.imshow(deberta_eval_gp, x=deberta_eval_gp.columns, y=deberta_eval_gp.index, \\\n",
    "                color_continuous_scale=px.colors.sequential.Greens, text_auto=True)\n",
    "fig.update_layout(width=500,height=500)\n",
    "fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207222f3-a419-407a-8c94-113b9c3ecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the accuracy \n",
    "deberta_accuracy = deberta_eval_df[deberta_eval_df['Model Label']==deberta_eval_df['Manual Label']]['review_id'].count()/\\\n",
    "            deberta_eval_df['review_id'].count()\n",
    "\n",
    "print('Accuracy of the model (based on manual evaluation):', format(deberta_accuracy, \".1%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f235d-147a-4b3c-9021-6d971eac3ab0",
   "metadata": {},
   "source": [
    "## Evaluation of SetFit ABSA Model results\n",
    "\n",
    "Here we are synthesizing the results from the SetFit ABSA model along with the manual evaluation done on 100 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85b15a-dc55-4323-a6e0-804bea4511ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data set which includes both manual evaluated results and the model labeled results\n",
    "setfit_absa_eval_df = pd.read_csv('SetFit_ABSA_manual_eval.csv')\n",
    "\n",
    "# let's look at the top row. For a given aspect, 'aspects_extracted_model_label' shows if the model mentioned there were aspects \n",
    "# and 'has_aspects_manual_label' is a field which is manually labelled and it captures if aspects were extracted correctly\n",
    "# by the model. There is Model label and Manual label which captures the sentiment for the aspect which was specified by the model\n",
    "# and the one manually label by us\n",
    "\n",
    "setfit_absa_eval_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7b67e-81bd-4a86-b1b8-ffd1fdb11d38",
   "metadata": {},
   "source": [
    "### Evaluating if Aspects were extracted when they were present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f35b6-edf3-424b-88f3-5036591ee718",
   "metadata": {},
   "source": [
    "Let's first look at if aspects were extracted when they were present and we will use confusion matrix for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc967ec-67a8-4e36-bfe8-5555bdae6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe to create a confusion matrix with model label and manual label\n",
    "setfit_absa_aspect_eval_gp = setfit_absa_eval_df.groupby(['has_aspects_manual_label','aspects_extracted_model_label'])\\\n",
    "                                                            ['review_id'].count().reset_index()\n",
    "\n",
    "setfit_absa_aspect_eval_gp = setfit_absa_aspect_eval_gp.pivot(index='has_aspects_manual_label', \\\n",
    "                                                              columns='aspects_extracted_model_label')['review_id'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870dac0-62f6-4493-8e06-5e2b825e35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the image using Plotly express heatmap\n",
    "fig = px.imshow(setfit_absa_aspect_eval_gp, x = setfit_absa_aspect_eval_gp.columns, y = setfit_absa_aspect_eval_gp.index, \\\n",
    "                color_continuous_scale=px.colors.sequential.Greens, text_auto=True)\n",
    "\n",
    "fig.update_layout(width=500,height=500)\n",
    "\n",
    "fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e195e-4a27-4e03-a010-e6a69b92dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the accuracy \n",
    "setfit_absa_aspect_accuracy = setfit_absa_eval_df[setfit_absa_eval_df['has_aspects_manual_label']==setfit_absa_eval_df['aspects_extracted_model_label']]\\\n",
    "                                    ['review_id'].count()/setfit_absa_eval_df['review_id'].count()\n",
    "\n",
    "print('Accuracy of the Setfit ABSA model (to ):', format(setfit_absa_aspect_accuracy, \".1%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d3d8d-929f-42a2-bba0-f4137b122bca",
   "metadata": {},
   "source": [
    "### Evaluating if Aspects extracted were correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f63c66-0982-4e67-986c-0ca1b195a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a filtered data set which only includes reviews with aspect and sentiment when aspect was extract \n",
    "# by the model in cases when it was present \n",
    "filtered_setfit_absa_df = setfit_absa_eval_df[(setfit_absa_eval_df['has_aspects_manual_label']=='Y') & \\\n",
    "                            (setfit_absa_eval_df['aspects_extracted_model_label']=='Y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54058ac-f2fc-4f6f-b4a2-662df6cdbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe to create a confusion matrix with model label and manual label\n",
    "setfit_absa_eval_gp = filtered_setfit_absa_df.groupby(['Model Label','Manual Label'])\\\n",
    "                            ['review_id'].count().reset_index()\n",
    "\n",
    "setfit_absa_eval_gp = setfit_absa_eval_gp.pivot(index='Model Label', columns='Manual Label')['review_id'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db9f50-d1aa-42cd-87a4-b980439b1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the image using Plotly express heatmap\n",
    "fig = px.imshow(setfit_absa_eval_gp, x = setfit_absa_eval_gp.columns, y = setfit_absa_eval_gp.index, \\\n",
    "                color_continuous_scale=px.colors.sequential.Greens, text_auto=True)\n",
    "\n",
    "fig.update_layout(width=500,height=500)\n",
    "\n",
    "fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e00b0b-845b-44bb-b76e-d7e4b8a2b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the accuracy \n",
    "setfit_absa_accuracy = filtered_setfit_absa_df[filtered_setfit_absa_df['Model Label']==filtered_setfit_absa_df['Manual Label']]['review_id'].count()/\\\n",
    "            filtered_setfit_absa_df['review_id'].count()\n",
    "\n",
    "print('Accuracy of the Setfit ABSA model (based on manual evaluation):', format(setfit_absa_accuracy, \".1%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea5a27-32e6-4644-9c68-9a1a99688f31",
   "metadata": {},
   "source": [
    "## Comparison of results between SetFit ABSA and Deberta models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8a826-6b0a-4e44-8d58-f638a2f04bc3",
   "metadata": {},
   "source": [
    "Now that we have evaluated results from SetFit ABSA and Deberta models, let's compare the results between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ea9f0-72f4-46ce-839d-35e06ce7e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SetFit ABSA data set which has the inference from the mdoel for 2000 samples of data\n",
    "temp_comp_setfit_absa = pd.read_csv('SetFit_ABSA_manual_eval_comparison.csv')\n",
    "temp_comp_setfit_absa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd12f2-8985-4286-bb48-471689b9b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "setfit_columns = ['atmosphere', 'food', 'service', 'seating', 'experience', 'setting', 'outdoor setting', 'location', \\\n",
    "                  'quality food', 'food quality', 'food quality side', 'weather', 'services', 'environment',\\\n",
    "                   'food atmosphere', 'family atmosphere', 'experiences', 'food quality way', 'seating area', 'scenery', 'price']\n",
    "\n",
    "comp_setfit_absa = pd.DataFrame()\n",
    "for cols in setfit_columns:\n",
    "    \n",
    "    temp_comp_setfit_absa['Aspect'] = cols\n",
    "    if len(comp_setfit_absa)==0:\n",
    "        temp_setfit = temp_comp_setfit_absa[['review_id','Aspect',cols]]\n",
    "        temp_setfit = temp_setfit.rename(columns={cols : 'Model Label'})\n",
    "        comp_setfit_absa = temp_setfit\n",
    "    else:\n",
    "        temp_setfit = temp_comp_setfit_absa[['review_id','Aspect',cols]]\n",
    "        temp_setfit = temp_setfit.rename(columns={cols : 'Model Label'})\n",
    "        comp_setfit_absa = pd.concat([comp_setfit_absa, temp_setfit], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8172974-04be-4902-bce0-211c3097fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_setfit_absa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cd934-a5f8-4ace-82b9-389e4866289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp_setfit_absa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4b8c5-3932-46e9-851b-240eae912618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SetFit ABSA data set which has the inference from the mdoel for 2000 samples of data\n",
    "temp_comp_deberta_absa = pd.read_csv('deberta_aspect_score_sentiment_results.csv')\n",
    "temp_comp_deberta_absa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f0937-91d3-4380-bea7-d5a52d594e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734beca-dbd9-400c-9351-f6049032db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global variables to store results\n",
    "global comp_deberta_absa \n",
    "comp_deberta_absa = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84d5cd-252d-416b-9667-dd120c4c527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSentimentPerReview(temp_df, aspect):\n",
    "    \"\"\" \n",
    "    Purpose of the function is to find sentiment per reviews per aspect \n",
    "    For each review per aspect:\n",
    "    - the label is marked negative, if the confidence score is >=0.7 and the label is negative\n",
    "    - the label is marked positive, if the confidence score is >=0.7 and the label is positive\n",
    "\n",
    "    Input: Dataframe with aspect label and aspect score for each review id\n",
    "    Output: Dataframe with sentiment for all aspects and for the sample reviews\n",
    "    \"\"\"\n",
    "    global comp_deberta_absa \n",
    "    \n",
    "    label_col = aspect+'_label'\n",
    "    score_col = aspect+'_score'\n",
    "\n",
    "    temp_df['Aspect'] = aspect\n",
    "    \n",
    "    # For each review and aspect, the label is marked negative, if the confidence score is >=0.7 and the label is negative\n",
    "    temp_df['Model Label'] = temp_df.apply(lambda x: 1 if (x[label_col]=='Negative') & \\\n",
    "                                        (x[score_col] >= 0.7) \\\n",
    "                                        else 'Positive' if (x[label_col]=='Positive') & (x[score_col] >= 0.7) else 'Neutral', axis=1)\n",
    "    if len(comp_deberta_absa)==0:\n",
    "        comp_deberta_absa = temp_df[['review_id','Aspect','Model Label']]\n",
    "    else:\n",
    "        comp_deberta_absa = pd.concat([comp_deberta_absa, temp_df[['review_id','Aspect','Model Label']]], ignore_index=True)\n",
    "\n",
    "    # return the final data frame\n",
    "    return comp_deberta_absa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be566f-06ef-4de7-88b1-489f5542ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the aspect columns to consider \n",
    "columns = ['ambiance','atmosphere','dessert','drinks','entertainment','experience','food','food_portion','kid_friendly',\\\n",
    "           'location','noise_level', 'price','seating','service','setting','space','waiting_time']\n",
    "\n",
    "# find sentiment per aspect for all reviews\n",
    "for aspect in columns:\n",
    "    findSentimentPerReview(temp_comp_deberta_absa[['review_id',aspect+'_label', aspect+'_score']], aspect)\n",
    "\n",
    "# Print the result\n",
    "comp_deberta_absa.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89a834-e348-4271-a485-995d164d25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp_deberta_absa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ecdfb-babc-4897-95e7-a042eb7c6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp_setfit_absa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a69826-3c4e-4557-b074-4183f32216e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two model results using aspect and review id as keys\n",
    "deberta_setfit_absa_df = pd.merge(comp_deberta_absa[comp_deberta_absa['Model Label']!=1], comp_setfit_absa, on=['Aspect','review_id'], how='left')\n",
    "\n",
    "deberta_setfit_absa_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ed46b-b6a7-4887-99ca-6a8ff669f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names and map it to the Deberta and SetFit ABSA column names \n",
    "deberta_setfit_absa_df = deberta_setfit_absa_df.rename(columns={'Model Label_x' : 'Deberta_Model_Results', 'Model Label_y' : 'Setfit_ABSA_Model_Results'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2822e8-3da1-48b4-9cb7-3ed0a1b2bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe to create a confusion matrix with Deberta model label and SetFit ABSA model label\n",
    "\n",
    "deberta_setfit_absa_gp = deberta_setfit_absa_df.groupby(['Deberta_Model_Results','Setfit_ABSA_Model_Results'])\\\n",
    "                                                            ['review_id'].count().reset_index()\n",
    "\n",
    "deberta_setfit_absa_gp = deberta_setfit_absa_gp.pivot(index='Deberta_Model_Results', \\\n",
    "                                                              columns='Setfit_ABSA_Model_Results')['review_id'].fillna(0)\n",
    "\n",
    "deberta_setfit_absa_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081a28c-122b-44b0-a88c-6453ceb76a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the image using Plotly express heatmap\n",
    "fig = px.imshow(deberta_setfit_absa_gp, x = deberta_setfit_absa_gp.columns, y = deberta_setfit_absa_gp.index, \\\n",
    "                color_continuous_scale=px.colors.sequential.Greens, text_auto=True)\n",
    "\n",
    "fig.update_layout(width=500,height=500)\n",
    "\n",
    "fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf12e03-1eee-4008-8d41-40486ad563a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
