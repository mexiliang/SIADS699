{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001afd04",
   "metadata": {},
   "source": [
    "## Data Exploration and Processing\n",
    "We downloaded Yelp dataset from https://www.yelp.com/dataset, the data is a zip file so we had to unzip the file and got our dataset in JSON format. /yelp_dataset is the directory of all the downloaded Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640116d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ec240",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file = 'yelp_dataset/yelp_academic_dataset_review.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b921288",
   "metadata": {},
   "source": [
    "Due to the size of the dataset and the dataset format, we decided to convert the file from JSON to csv. Below lines of code is to perform the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# converting reviews dataset\n",
    "chunks = pd.read_json(review_file, lines=True, chunksize = 10000)\n",
    "reviews = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    reviews = reviews.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896fb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's faster to write the results in a csv using pyarrow\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "\n",
    "out = pa.Table.from_pandas(reviews)\n",
    "file_name = 'yelp_reviews.csv'\n",
    "# uncomment below lines to write output to a csv\n",
    "# csv.write_csv(out, file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the newly saved file\n",
    "reviews_df = pd.read_csv('yelp_reviews.csv')\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e67742",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_file = 'yelp_dataset/yelp_academic_dataset_business.json'\n",
    "chunks = pd.read_json(business_file, lines=True, chunksize = 10000)\n",
    "business = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    business = business.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9612af",
   "metadata": {},
   "source": [
    "We are not only interested in the reviews but also want to learn about the business which people review about. Therefore, we need to merge review dataset and business dataset, joining on business id of a merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews_df.merge(business, on='business_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ed73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging reviews and business datasets\n",
    "# uncomment below lines to write to csv file\n",
    "# df_out = pa.Table.from_pandas(df)\n",
    "# file_name = 'yelp_data.csv'\n",
    "# csv.write_csv(out, file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cabc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring unique categories\n",
    "category_list = [i.split(', ') for i in df_pa['categories']]\n",
    "# first = category_list[0]\n",
    "uniques = set()\n",
    "for i in range(len(category_list)):\n",
    "    uniques.update(category_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe that contains reviews in California\n",
    "df_ca = df[df['state']=='CA']\n",
    "df_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['PA', 'AZ', 'LA', 'CA', 'FL', 'IN', 'MO', 'TN', 'NV', 'AB', 'NJ',\n",
    "       'IL', 'ID', 'DE', 'HI', 'NC', 'CO', 'WA', 'UT', 'TX', 'MT', 'MI',\n",
    "       'SD', 'XMS', 'MA', 'VI', 'VT']\n",
    "for state in state_list:\n",
    "    print('data records in {} is '.format(state), df[df['state']==state].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a3008",
   "metadata": {},
   "source": [
    "For our training purpose, we decided to go with restaurant reviews for California given the smaller data size and diversity of this state. Therefore, first we need to get all the unique labels for business in California and then later we need to manually include food related lable to filter out non-restaurants in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f28a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca.dropna(subset=['categories'], inplace=True)\n",
    "category_list_ca = [i.split(', ') for i in df_ca['categories']]\n",
    "# first = category_list[0]\n",
    "uniques_ca = set()\n",
    "for i in range(len(category_list_ca)):\n",
    "    uniques_ca.update(category_list_ca[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ca_df = pd.DataFrame(uniques_ca, columns=['labels'])\n",
    "out_ca_df['labels'] = uniques_ca\n",
    "# uncomment below line to write to a csv file\n",
    "# out_ca_df.to_csv('ca_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472032c",
   "metadata": {},
   "source": [
    "From our manual labeling process, we found out below list of food related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_labels =  ['Acai Bowls',\n",
    " 'American (New)',\n",
    " 'American (Traditional)',\n",
    " 'Arabic',\n",
    " 'Argentine',\n",
    " 'Asian Fusion',\n",
    " 'Australian',\n",
    " 'Bagels',\n",
    " 'Bakeries',\n",
    " 'Barbeque',\n",
    " 'Bars',\n",
    " 'Basque',\n",
    " 'Bed & Breakfast',\n",
    " 'Beer',\n",
    " 'Beer Bar',\n",
    " 'Beer Gardens',\n",
    " 'Belgian',\n",
    " 'Beverage Store',\n",
    " 'Brazilian',\n",
    " 'Breakfast & Brunch',\n",
    " 'Brewpubs',\n",
    " 'British',\n",
    " 'Bubble Tea',\n",
    " 'Buffets',\n",
    " 'Burgers',\n",
    " 'Cafes',\n",
    " 'Cafeteria',\n",
    " 'Cajun/Creole',\n",
    " 'Candy Stores',\n",
    " 'Cantonese',\n",
    " 'Caribbean',\n",
    " 'Caterers',\n",
    " 'Champagne Bars',\n",
    " 'Cheese Shops',\n",
    " 'Cheesesteaks',\n",
    " 'Chicken Shop',\n",
    " 'Chicken Wings',\n",
    " 'Chinese',\n",
    " 'Chocolatiers & Shops',\n",
    " 'Cocktail Bars',\n",
    " 'Coffee & Tea',\n",
    " 'Coffee Roasteries',\n",
    " 'Coffeeshops',\n",
    " 'Comfort Food',\n",
    " 'Creperies',\n",
    " 'Cuban',\n",
    " 'Cupcakes',\n",
    " 'Custom Cakes',\n",
    " 'Delicatessen',\n",
    " 'Delis',\n",
    " 'Desserts',\n",
    " 'Dim Sum',\n",
    " 'Diners',\n",
    " 'Dive Bars',\n",
    " 'Do-It-Yourself Food',\n",
    " 'Donuts',\n",
    " 'Empanadas',\n",
    " 'Ethiopian',\n",
    " 'Ethnic Food',\n",
    " 'Falafel',\n",
    " 'Fast Food',\n",
    " 'Fish & Chips',\n",
    " 'Fondue',\n",
    " 'Food',\n",
    " 'Food Court',\n",
    " 'Food Delivery Services',\n",
    " 'Food Stands',\n",
    " 'Food Tours',\n",
    " 'Food Trucks',\n",
    " 'French',\n",
    " 'Fruits & Veggies',\n",
    " 'Gay Bars',\n",
    " 'Gelato',\n",
    " 'German',\n",
    " 'Gluten-Free',\n",
    " 'Greek',\n",
    " 'Halal',\n",
    " 'Hawaiian',\n",
    " 'Herbs & Spices',\n",
    " 'Himalayan/Nepalese',\n",
    " 'Honey',\n",
    " 'Hong Kong Style Cafe',\n",
    " 'Hookah Bars',\n",
    " 'Hot Dogs',\n",
    " 'Hot Pot',\n",
    " 'Ice Cream & Frozen Yogurt',\n",
    " 'Imported Food',\n",
    " 'Indian',\n",
    " 'Indonesian',\n",
    " 'Internet Cafes',\n",
    " 'Irish',\n",
    " 'Irish Pub',\n",
    " 'Italian',\n",
    " 'Japanese',\n",
    " 'Juice Bars & Smoothies',\n",
    " 'Kebab',\n",
    " 'Kombucha',\n",
    " 'Korean',\n",
    " 'Latin American',\n",
    " 'Lebanese',\n",
    " 'Live/Raw Food',\n",
    " 'Lounges',\n",
    " 'Macarons',\n",
    " 'Mediterranean',\n",
    " 'Mexican',\n",
    " 'Middle Eastern',\n",
    " 'Modern European',\n",
    " 'Mongolian',\n",
    " 'Moroccan',\n",
    " 'Muay Thai',\n",
    " 'New Mexican Cuisine',\n",
    " 'Noodles',\n",
    " 'Pakistani',\n",
    " 'Pan Asian',\n",
    " 'Pasta Shops',\n",
    " 'Patisserie/Cake Shop',\n",
    " 'Persian/Iranian',\n",
    " 'Peruvian',\n",
    " 'Pizza',\n",
    " 'Poke',\n",
    " 'Pop-Up Restaurants',\n",
    " 'Pretzels',\n",
    " 'Pubs',\n",
    " 'Ramen',\n",
    " 'Restaurants',\n",
    " 'Salad',\n",
    " 'Salvadoran',\n",
    " 'Sandwiches',\n",
    " 'Scandinavian',\n",
    " 'Seafood',\n",
    " 'Shaved Ice',\n",
    " 'Soul Food',\n",
    " 'Soup',\n",
    " 'Southern',\n",
    " 'Spanish',\n",
    " 'Speakeasies',\n",
    " 'Specialty Food',\n",
    " 'Steakhouses',\n",
    " 'Sushi Bars',\n",
    " 'Szechuan',\n",
    " 'Tacos',\n",
    " 'Taiwanese',\n",
    " 'Tapas Bars',\n",
    " 'Tapas/Small Plates',\n",
    " 'Tasting Classes',\n",
    " 'Tea Rooms',\n",
    " 'Tex-Mex',\n",
    " 'Thai',\n",
    " 'Themed Cafes',\n",
    " 'Turkish',\n",
    " 'Tuscan',\n",
    " 'Vegan',\n",
    " 'Vegetarian',\n",
    " 'Vietnamese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to identify a restaurant\n",
    "def is_restaurant(list1, list2):\n",
    "    for item in list1:\n",
    "        if item in list2:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef95d8",
   "metadata": {},
   "source": [
    "Looking at each of the category in categories column of each row, if any of the categories matches our food list, then we will mark this business is a restaurant. Y for is a restaurant, N for a non-restaurant business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_res = []\n",
    "for categories in df_ca['categories']:\n",
    "    cate_list = categories.split(', ')\n",
    "    if is_restaurant(cate_list, food_labels):\n",
    "        is_res.append('Y')\n",
    "    else:\n",
    "        is_res.append('N')\n",
    "\n",
    "df_ca['is_restaurant'] = is_res\n",
    "df_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_res_df = df_ca[df_ca['is_restaurant'] =='Y']\n",
    "#  creating California restaurant dataset, uncomment below line to write a csv\n",
    "# ca_res_df.to_csv('ca_restaurants.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
