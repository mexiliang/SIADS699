{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0febdd7",
   "metadata": {
    "id": "5d440bc9-8526-4532-8126-94188dcd41dd"
   },
   "source": [
    "## SetFit ABSA Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43866ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38db62-2707-41d4-b1be-90ec6ee7c787",
   "metadata": {
    "id": "ed38db62-2707-41d4-b1be-90ec6ee7c787"
   },
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555ad2e-f7f4-4611-a337-ecbc75fafad9",
   "metadata": {
    "id": "9555ad2e-f7f4-4611-a337-ecbc75fafad9"
   },
   "outputs": [],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba22efe",
   "metadata": {},
   "source": [
    "CUDA is required to run SetFit ABSA model, run below code block to check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VX1sWC8bYlt0",
   "metadata": {
    "id": "VX1sWC8bYlt0"
   },
   "outputs": [],
   "source": [
    "# chekc if cuda is available\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a290ef",
   "metadata": {},
   "source": [
    "The training dataset we prepared for trainning our own SetFit ABSA model is made available through huggingface. \n",
    "https://huggingface.co/datasets/ginkgogo/ca_restaurants_random_sample We should be able to load the dataset directly from huggingface fter installing required setfit[absa] packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45583a6c-e8da-43a3-a5a0-80f208b2a06f",
   "metadata": {
    "id": "45583a6c-e8da-43a3-a5a0-80f208b2a06f"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ginkgogo/ca_restaurants_random_sample\", split=\"train\")\n",
    "# splitting dataset into two parts, one for training purposes and the other one for evaluation\n",
    "train_dataset = dataset.select(range(50))\n",
    "eval_dataset = dataset.select(range(50, 102))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SLIWAXvorYQf",
   "metadata": {
    "id": "SLIWAXvorYQf"
   },
   "outputs": [],
   "source": [
    "# quickly take a look at our training data\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2zEMPxprbq4",
   "metadata": {
    "id": "s2zEMPxprbq4"
   },
   "outputs": [],
   "source": [
    "# also spot on our evaluation data\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce783030",
   "metadata": {},
   "source": [
    "Prepare a new instance of Absa model, with selected transformers and spacy large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd84c9-7187-46d7-b96b-439a363a45b0",
   "metadata": {
    "id": "16bd84c9-7187-46d7-b96b-439a363a45b0"
   },
   "outputs": [],
   "source": [
    "from setfit import AbsaModel\n",
    "\n",
    "model = AbsaModel.from_pretrained(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    spacy_model=\"en_core_web_sm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4a414",
   "metadata": {},
   "source": [
    "### Training the SetFitABSA model\n",
    "Prepare training arguments for the ABSA model and passing training dataset and evaluation dataset to the training process. We completed the training using Google Colab and it took about 1 hour using A100 GPU run-time environment. Therefore, we saved this model to huggingface so that we can use it whenever we want without rerun the training. Check \"Using SetFitABSA model\" below for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232260c6-e858-42bd-b96d-101667e5c8e0",
   "metadata": {
    "id": "232260c6-e858-42bd-b96d-101667e5c8e0"
   },
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset, AbsaTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    num_epochs=5,\n",
    "    use_amp=True,\n",
    "    batch_size=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = AbsaTrainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a788cc4",
   "metadata": {},
   "source": [
    "In order to inspect the model, we use the built-in method provided by the setfit[absa] package to check the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T-TNnSBAkpR8",
   "metadata": {
    "id": "T-TNnSBAkpR8"
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(eval_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROdScmCnmLV-",
   "metadata": {
    "id": "ROdScmCnmLV-"
   },
   "outputs": [],
   "source": [
    "# pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225680b2",
   "metadata": {},
   "source": [
    "### Saving the SetFitABSA model to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qKAT6dPtmXYi",
   "metadata": {
    "id": "qKAT6dPtmXYi"
   },
   "outputs": [],
   "source": [
    "# uncomment below to login to huggingface\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SN3bKljwlif3",
   "metadata": {
    "id": "SN3bKljwlif3"
   },
   "outputs": [],
   "source": [
    "# uncomment below to save the model to huggingface\n",
    "# model.push_to_hub(\"ginkgogo/setfit-absa-bge-small-en-v1.5-restaurants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d43f1d",
   "metadata": {},
   "source": [
    "### Using SetFitABSA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8Pkp-hqrENK",
   "metadata": {
    "id": "h8Pkp-hqrENK"
   },
   "outputs": [],
   "source": [
    "from setfit import AbsaModel\n",
    "\n",
    "# Download from the ðŸ¤— Hub\n",
    "model = AbsaModel.from_pretrained(\n",
    "    \"ginkgogo/setfit-absa-bge-small-en-v1.5-restaurants-aspect\",\n",
    "    \"ginkgogo/setfit-absa-bge-small-en-v1.5-restaurants-polarity\",\n",
    "    spacy_model=\"en_core_web_sm\",\n",
    ")\n",
    "# Run inference\n",
    "preds = model(\"The food was great, but the venue is just way too busy.\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LGXqKnN3rU-b",
   "metadata": {
    "id": "LGXqKnN3rU-b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "  '/content/drive/MyDrive/699/ca_restaurants.csv'\n",
    ")\n",
    "# this is list of business ids that we used in training the SetFit ABSA model, \n",
    "# we need to ommit this from the random sample to avoid bias\n",
    "bus_used_in_train = [234152, 88955, 174286, 228338, 203671, 151156, 88166, 64932, 142804, 210180, 35159, 90839, 137484, 85880, 128479, 92603, 20842, 200330, 175440, 8844, 61777, 3815, 123379, 125840, 180129, 206443, 219869, 101729, 107887, 188230, 244420, 49208, 139902, 242337, 35581, 228649, 44946, 32763, 69556, 152494, 5069963, 3915492, 4486491]\n",
    "\n",
    "random_df_2000 = df.sample(2000)\n",
    "\n",
    "for business in bus_used_in_train:\n",
    "    if business in random_df_2000['business_id']:\n",
    "        random_df_2000.drop(business, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42kb7xANz7oi",
   "metadata": {
    "id": "42kb7xANz7oi"
   },
   "outputs": [],
   "source": [
    "# run inference on the random sample of 2k rows from the California resturant dataset\n",
    "sentences = list(random_df_2000['text'].str.lower())\n",
    "preds = model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-nsFgsx_zpNu",
   "metadata": {
    "id": "-nsFgsx_zpNu"
   },
   "outputs": [],
   "source": [
    "# quickly inspect model predictions\n",
    "print(preds)\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be0bcc",
   "metadata": {},
   "source": [
    "Model prediction (preds) is a list of lists of aspect and sentiment pair like this [[{'span': 'food', 'polarity': 'positive'}],\n",
    "[{'span': 'food', 'polarity': 'positive'}, {'span': 'prices', 'polarity': 'positive'}],\n",
    "[{'span': 'waiting time', 'polarity': 'positive'}]] To better view the aspect and sentiment, we need to make dictionary of aspect: sentiment like this {'food': 'positive', 'service': 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shAW8oCr5B-F",
   "metadata": {
    "id": "shAW8oCr5B-F"
   },
   "outputs": [],
   "source": [
    "# if there's no sentiment extracted, use empty {} as the column value\n",
    "aspects_sentiment = []\n",
    "for i in preds:\n",
    "    if len(i) > 0:\n",
    "        aspects_sentiment.append(i)\n",
    "    else:\n",
    "        aspects_sentiment.append('{}')\n",
    "\n",
    "random_df_2000['aspects_sentiment'] = aspects_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmcYLrMC8TvY",
   "metadata": {
    "id": "bmcYLrMC8TvY"
   },
   "outputs": [],
   "source": [
    "# method to create aspect: sentiment dict\n",
    "def extract(aspect_list):\n",
    "    if isinstance(aspect_list, list):\n",
    "        aspect_dict = {}\n",
    "    for aspect in aspect_list:\n",
    "        aspect_dict[aspect['span']] = aspect['polarity']\n",
    "    return aspect_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dENJQ9rS68f0",
   "metadata": {
    "id": "dENJQ9rS68f0"
   },
   "outputs": [],
   "source": [
    "# apply this method to the random samples\n",
    "random_df_2000['aspects_sentiment'] = random_df_2000['aspects_sentiment'].apply(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XS0ipP3E-ENY",
   "metadata": {
    "id": "XS0ipP3E-ENY"
   },
   "outputs": [],
   "source": [
    "random_df_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58xQBBYiDmoo",
   "metadata": {
    "id": "58xQBBYiDmoo"
   },
   "outputs": [],
   "source": [
    "# create a dataframe where one or more aspects are extracted from the review text\n",
    "with_aspect_df = random_df_2000.dropna(subset=['aspects_sentiment'])\n",
    "print(with_aspect_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_G91_LC2IShc",
   "metadata": {
    "id": "_G91_LC2IShc"
   },
   "outputs": [],
   "source": [
    "# prepare this random sample for a file\n",
    "\n",
    "random_df_2000['aspects_sentiment'] = random_df_2000['aspects_sentiment'].fillna('{}')\n",
    "random_df_2000['aspects_sentiment'] = random_df_2000['aspects_sentiment'].apply(ast.literal_eval)\n",
    "flatten_asepct = pd.json_normalize(random_df_2000['aspects_sentiment'])\n",
    "\n",
    "random_df_2000.reset_index(inplace=True)\n",
    "flatten_asepct.reset_index(inplace=True)\n",
    "final_df = pd.concat([random_df_2000, flatten_asepct], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favPPeD3HkLe",
   "metadata": {
    "id": "favPPeD3HkLe"
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cLhWuToBHFpW",
   "metadata": {
    "id": "cLhWuToBHFpW"
   },
   "outputs": [],
   "source": [
    "# prepare this random sample with aspects extracted for a file\n",
    "with_aspect_df.reset_index(inplace=True)\n",
    "with_aspect_df['aspects_sentiment'] = with_aspect_df['aspects_sentiment'].apply(ast.literal_eval)\n",
    "with_aspect_flatten = pd.json_normalize(with_aspect_df['aspects_sentiment'])\n",
    "with_aspect_flatten.reset_index(inplace=True)\n",
    "with_aspect_final_df = pd.concat([with_aspect_df, with_aspect_flatten], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2fb0c",
   "metadata": {},
   "source": [
    "### Saving processed data to drive\n",
    "We saved processed data to csv for us to better perform manual evaluation. Uncomment sections of code to create files to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below lines to mount to your Google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "YOf2NrBMCMKX",
   "metadata": {
    "id": "YOf2NrBMCMKX"
   },
   "outputs": [],
   "source": [
    "# uncomment below lines to write to drive\n",
    "# with_aspect_final_df.to_csv('with_aspect_from_random_2k.csv')\n",
    "# !cp with_aspect_from_random_2k.csv '/content/drive/MyDrive/699/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Geo8liLoE5VK",
   "metadata": {
    "id": "Geo8liLoE5VK"
   },
   "outputs": [],
   "source": [
    "# final_df.to_csv('random_2k.csv')\n",
    "# !cp random_2k.csv '/content/drive/MyDrive/699/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bAPylHy3FIqh",
   "metadata": {
    "id": "bAPylHy3FIqh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out = pd.read_csv('/content/drive/MyDrive/699/with_aspect_from_random_2k.csv')\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZlF0fBiPJEgT",
   "metadata": {
    "id": "ZlF0fBiPJEgT"
   },
   "outputs": [],
   "source": [
    "out_random = pd.read_csv('/content/drive/MyDrive/699/random_2k.csv')\n",
    "out_random.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eac757",
   "metadata": {},
   "source": [
    "### Reading Manual evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9317198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load manual evaluation results \n",
    "import pandas as pd\n",
    "setfit_absa_eval_df = pd.read_csv('../data/results/SetFit_ABSA_manual_eval.csv')\n",
    "setfit_absa_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3edf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "setfit_absa_eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate when there's an aspect extracted, the accuray of predicting the correct sentiment\n",
    "cal_df = setfit_absa_eval_df[(setfit_absa_eval_df['has_aspects_model_label'] == 'Y') \n",
    "                             & (setfit_absa_eval_df['aspects_extracted_manual_label'] == 'Y')]\n",
    "sentiment_correctness = len(cal_df[cal_df['Model Label'] == cal_df['Manual Label']])/len(cal_df)\n",
    "print('Accuracy of predicting sentiment is :', format(sentiment_correctness, \".1%\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
